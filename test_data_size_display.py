#!/usr/bin/env python3
"""
Test script to verify data size display functionality.
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime

def create_test_data_file(size_mb=1.0):
    """Create a test data file of specified size."""
    # Calculate number of rows needed for target size
    # Each row has ~100 bytes (7 columns * ~14 bytes each)
    bytes_per_row = 100
    target_bytes = size_mb * 1024 * 1024
    n_rows = int(target_bytes / bytes_per_row)
    
    print(f"Creating test data file with {n_rows:,} rows for {size_mb:.1f} MB")
    
    # Create synthetic stock data - use simple integer index instead of dates for large datasets
    if n_rows > 10000:
        # For large datasets, use simple integer index to avoid date range issues
        dates = range(n_rows)
    else:
        # For smaller datasets, use proper dates
        dates = pd.date_range(start='2020-01-01', periods=n_rows, freq='D')
    
    # Generate realistic stock data
    base_price = 100
    price_changes = np.random.normal(0, 1, n_rows)
    prices = [base_price]
    
    for change in price_changes[1:]:
        new_price = prices[-1] * (1 + change * 0.02)  # 2% daily volatility
        prices.append(max(new_price, 1))  # Ensure price doesn't go negative
    
    # Create OHLCV data
    data = {
        'date': dates,
        'open': prices,
        'high': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
        'low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
        'close': prices,
        'volume': np.random.randint(1000000, 10000000, n_rows)
    }
    
    df = pd.DataFrame(data)
    
    # Add some technical indicators
    df['ma_5'] = df['close'].rolling(window=5).mean()
    df['ma_10'] = df['close'].rolling(window=10).mean()
    df['rsi'] = 50 + np.random.normal(0, 10, n_rows)  # Simplified RSI
    
    # Clean up NaN values
    df = df.dropna()
    
    return df

def test_data_size_display():
    """Test the data size display functionality."""
    print("üß™ Testing Data Size Display Functionality")
    print("=" * 50)
    
    try:
        # Test different file sizes
        test_sizes = [0.1, 1.0, 10.0, 100.0]  # MB
        
        for size_mb in test_sizes:
            print(f"\nüìä Testing {size_mb:.1f} MB data file...")
            
            # Create test data
            df = create_test_data_file(size_mb)
            
            # Save to temporary file
            test_file = f"test_data_{size_mb:.1f}mb.csv"
            df.to_csv(test_file, index=False)
            
            # Check file size
            file_size_bytes = os.path.getsize(test_file)
            if file_size_bytes > 1024 * 1024 * 1024:  # > 1GB
                file_size_str = f"{file_size_bytes / (1024**3):.2f} GB"
            else:
                file_size_str = f"{file_size_bytes / (1024**2):.2f} MB"
            
            print(f"   üìÅ File size on disk: {file_size_str}")
            
            # Check loaded data size
            df_loaded = pd.read_csv(test_file)
            data_size_bytes = df_loaded.memory_usage(deep=True).sum()
            if data_size_bytes > 1024 * 1024 * 1024:  # > 1GB
                data_size_str = f"{data_size_bytes / (1024**3):.2f} GB"
            else:
                data_size_str = f"{data_size_bytes / (1024**2):.2f} MB"
            
            print(f"   üìè Loaded data size in memory: {data_size_str}")
            
            # Check if data is too large
            if data_size_bytes > 500 * 1024 * 1024:  # > 500MB
                print(f"   ‚ö†Ô∏è Large dataset detected - would trigger batch size reduction")
            else:
                print(f"   ‚úÖ Dataset size is manageable")
            
            # Cleanup
            os.remove(test_file)
            print(f"   üßπ Cleaned up {test_file}")
        
        print(f"\nüéâ All data size display tests passed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_memory_optimization():
    """Test memory optimization logic."""
    print("\nüß™ Testing Memory Optimization Logic")
    print("=" * 40)
    
    try:
        # Test different data sizes and batch size adjustments
        test_cases = [
            (0.1, 32, 32),    # Small data, no change
            (1.0, 32, 32),    # Medium data, no change
            (100.0, 32, 32),  # Large data, but under 500MB threshold
            (600.0, 32, 16),  # Very large data, over 500MB threshold
            (500.0, 64, 16),  # Very large data, over 500MB threshold
        ]
        
        for data_size_mb, original_batch_size, expected_batch_size in test_cases:
            print(f"\nüìä Testing {data_size_mb:.1f} MB data with batch size {original_batch_size}...")
            
            # Simulate the logic from _train_model
            data_size_bytes = data_size_mb * 1024 * 1024
            batch_size = original_batch_size
            
            if data_size_bytes > 500 * 1024 * 1024:  # > 500MB
                batch_size = min(batch_size, 16)
                print(f"   ‚ö†Ô∏è Large dataset detected - reducing batch size")
                print(f"   üîÑ Batch size: {original_batch_size} ‚Üí {batch_size}")
            else:
                print(f"   ‚úÖ Dataset size is manageable - keeping batch size {batch_size}")
            
            # Verify the logic
            if batch_size == expected_batch_size:
                print(f"   ‚úÖ Memory optimization logic working correctly")
            else:
                print(f"   ‚ùå Expected batch size {expected_batch_size}, got {batch_size}")
        
        print(f"\nüéâ All memory optimization tests passed!")
        return True
        
    except Exception as e:
        print(f"\n‚ùå Test failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("üß™ Data Size Display and Memory Optimization Test Suite")
    print("=" * 60)
    
    success1 = test_data_size_display()
    success2 = test_memory_optimization()
    
    if success1 and success2:
        print("\n‚úÖ All tests passed! Data size display and memory optimization are working correctly.")
        print("\nüìã Summary:")
        print("   ‚Ä¢ Data file size is displayed in MB/GB before training")
        print("   ‚Ä¢ Loaded data size in memory is calculated and displayed")
        print("   ‚Ä¢ Large datasets (>500MB) trigger automatic batch size reduction")
        print("   ‚Ä¢ Memory optimization helps prevent training crashes")
        sys.exit(0)
    else:
        print("\n‚ùå Some tests failed!")
        sys.exit(1) 